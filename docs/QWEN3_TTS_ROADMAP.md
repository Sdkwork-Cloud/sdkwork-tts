# Qwen3-TTS ç”Ÿäº§å°±ç»ªè·¯çº¿å›¾

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†å°†å½“å‰ Qwen3-TTS Rust å®ç°å˜ä¸ºç”Ÿäº§å°±ç»ª TTS å¼•æ“çš„è¯¦ç»†è·¯çº¿å›¾å’Œå®ç°æŒ‡å—ã€‚

**å½“å‰çŠ¶æ€**: âœ… æ¶æ„å®Œæ•´ï¼Œæµ‹è¯•é€šè¿‡  
**ç›®æ ‡çŠ¶æ€**: ğŸ¯ ç”Ÿäº§å°±ç»ªï¼Œå®Œæ•´æ¨ç†  
**é¢„è®¡å·¥ä½œé‡**: ~1,700 è¡Œä»£ç ï¼Œ2-3 å‘¨å¼€å‘æ—¶é—´

---

## é˜¶æ®µ 1: æ¨¡å‹æƒé‡åŠ è½½ (500 è¡Œ)

### 1.1 HuggingFace æƒé‡ä¸‹è½½

#### ä»»åŠ¡
- [ ] å®ç°æ¨¡å‹ä¸‹è½½å™¨
- [ ] æ”¯æŒæ–­ç‚¹ç»­ä¼ 
- [ ] éªŒè¯æ–‡ä»¶å®Œæ•´æ€§

#### å®ç°æŒ‡å—

```rust
// src/models/qwen3_tts/downloader.rs

use hf_hub::{api::sync::Api, Repo, RepoType};
use std::path::{Path, PathBuf};

pub struct ModelDownloader {
    api: Api,
    cache_dir: PathBuf,
}

impl ModelDownloader {
    pub fn new(cache_dir: Option<PathBuf>) -> Result<Self> {
        let api = Api::new()?;
        let cache_dir = cache_dir.unwrap_or_else(|| {
            dirs::home_dir()
                .unwrap()
                .join(".cache")
                .join("qwen3-tts")
        });
        Ok(Self { api, cache_dir })
    }

    pub fn download(&self, model_id: &str) -> Result<PathBuf> {
        let repo = Repo::with_revision(
            model_id.to_string(),
            RepoType::Model,
            "main".to_string(),
        );
        
        let model_dir = self.api.repo(repo).get(".")?;
        Ok(model_dir)
    }

    pub fn download_file(&self, model_id: &str, filename: &str) -> Result<PathBuf> {
        let repo = Repo::with_revision(
            model_id.to_string(),
            RepoType::Model,
            "main".to_string(),
        );
        
        let file_path = self.api.repo(repo).get(filename)?;
        Ok(file_path)
    }
}
```

#### æµ‹è¯•
```rust
#[test]
fn test_download() {
    let downloader = ModelDownloader::new(None).unwrap();
    let path = downloader.download("Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice").unwrap();
    assert!(path.exists());
}
```

### 1.2 æƒé‡æ˜ å°„

#### ä»»åŠ¡
- [ ] å®šä¹‰æƒé‡æ˜ å°„è¡¨
- [ ] å®ç°æƒé‡åŠ è½½å™¨
- [ ] éªŒè¯æƒé‡å®Œæ•´æ€§

#### æƒé‡æ˜ å°„è¡¨

```rust
// src/models/qwen3_tts/weights.rs

use std::collections::HashMap;

pub fn get_talker_weight_map() -> HashMap<String, String> {
    let mut map = HashMap::new();
    
    // Embedding
    map.insert("model.embed_tokens.weight".to_string(), "embed_tokens.weight".to_string());
    
    // Transformer layers
    for i in 0..28 {
        let prefix = format!("model.layers.{i}");
        
        // Self-attention
        map.insert(
            format!("{prefix}.self_attn.q_proj.weight"),
            format!("{prefix}.self_attn.wq.weight"),
        );
        map.insert(
            format!("{prefix}.self_attn.k_proj.weight"),
            format!("{prefix}.self_attn.wk.weight"),
        );
        map.insert(
            format!("{prefix}.self_attn.v_proj.weight"),
            format!("{prefix}.self_attn.wv.weight"),
        );
        map.insert(
            format!("{prefix}.self_attn.o_proj.weight"),
            format!("{prefix}.self_attn.wo.weight"),
        );
        
        // MLP
        map.insert(
            format!("{prefix}.mlp.gate_proj.weight"),
            format!("{prefix}.mlp.w1.weight"),
        );
        map.insert(
            format!("{prefix}.mlp.up_proj.weight"),
            format!("{prefix}.mlp.w3.weight"),
        );
        map.insert(
            format!("{prefix}.mlp.down_proj.weight"),
            format!("{prefix}.mlp.w2.weight"),
        );
        
        // Layer norms
        map.insert(
            format!("{prefix}.input_layernorm.weight"),
            format!("{prefix}.ln1.weight"),
        );
        map.insert(
            format!("{prefix}.post_attention_layernorm.weight"),
            format!("{prefix}.ln2.weight"),
        );
    }
    
    // Final norm
    map.insert("model.norm.weight".to_string(), "norm.weight".to_string());
    
    map
}
```

#### æƒé‡åŠ è½½å™¨

```rust
use candle_core::{safetensors::load, Device, Tensor};
use std::collections::HashMap;
use std::path::Path;

pub struct WeightLoader {
    weights: HashMap<String, Tensor>,
}

impl WeightLoader {
    pub fn load<P: AsRef<Path>>(path: P, device: &Device) -> Result<Self> {
        let weights = load(path.as_ref(), device)?;
        Ok(Self { weights })
    }

    pub fn get(&self, name: &str) -> Result<&Tensor> {
        self.weights.get(name).ok_or_else(|| {
            anyhow::anyhow!("Weight not found: {}", name)
        })
    }

    pub fn remap(&self, mapping: &HashMap<String, String>) -> HashMap<String, Tensor> {
        let mut remapped = HashMap::new();
        
        for (hf_name, our_name) in mapping {
            if let Some(tensor) = self.weights.get(hf_name) {
                remapped.insert(our_name.clone(), tensor.clone());
            }
        }
        
        remapped
    }

    pub fn verify(&self, expected_keys: &[&str]) -> Result<()> {
        for key in expected_keys {
            if !self.weights.contains_key(*key) {
                return Err(anyhow::anyhow!("Missing weight: {}", key));
            }
        }
        Ok(())
    }
}
```

### 1.3 Safetensors æ ¼å¼æ”¯æŒ

#### ä»»åŠ¡
- [ ] å®ç° safetensors åŠ è½½
- [ ] æ”¯æŒåˆ†ç‰‡åŠ è½½
- [ ] å†…å­˜æ˜ å°„ä¼˜åŒ–

```rust
use safetensors::SafeTensors;
use memmap2::Mmap;
use std::fs::File;

pub fn load_safetensors_mmap<P: AsRef<Path>>(
    path: P,
    device: &Device,
) -> Result<HashMap<String, Tensor>> {
    let file = File::open(path.as_ref())?;
    let mmap = unsafe { Mmap::map(&file)? };
    
    let safetensor = SafeTensors::deserialize(&mmap)?;
    
    let mut tensors = HashMap::new();
    for name in safetensor.names() {
        let view = safetensor.tensor(&name)?;
        let tensor = Tensor::from_raw_buffer(
            view.data(),
            view.dtype().into(),
            view.shape(),
            device,
        )?;
        tensors.insert(name.to_string(), tensor);
    }
    
    Ok(tensors)
}
```

---

## é˜¶æ®µ 2: å®Œæ•´æ¨ç†å¾ªç¯ (300 è¡Œ)

### 2.1 é›†æˆ TalkerModel + CodePredictor + Decoder

#### ä»»åŠ¡
- [ ] åˆ›å»ºæ¨ç†ç®¡é“
- [ ] å®ç°æ•°æ®æµè½¬æ¢
- [ ] é”™è¯¯å¤„ç†

```rust
// src/models/qwen3_tts/pipeline.rs

pub struct Qwen3TTSPipeline {
    talker: TalkerModel,
    code_predictor: CodePredictor,
    decoder: Decoder12Hz,
    device: Device,
}

impl Qwen3TTSPipeline {
    pub fn new(
        talker: TalkerModel,
        code_predictor: CodePredictor,
        decoder: Decoder12Hz,
        device: Device,
    ) -> Self {
        Self {
            talker,
            code_predictor,
            decoder,
            device,
        }
    }

    pub fn synthesize(
        &self,
        text_ids: &[u32],
        speaker_condition: Option<&Tensor>,
        config: &GenerationConfig,
    ) -> Result<Vec<f32>> {
        // Step 1: TalkerModel - text to semantic tokens
        let semantic_tokens = self.talker.generate(text_ids, speaker_condition, config)?;
        
        // Step 2: CodePredictor - semantic to acoustic codes
        let acoustic_codes = self.code_predictor.generate(&semantic_tokens)?;
        
        // Step 3: Decoder - acoustic codes to audio
        let audio = self.decoder.decode(&acoustic_codes)?;
        
        Ok(audio)
    }
}
```

### 2.2 è‡ªå›å½’ç”Ÿæˆå¾ªç¯

#### ä»»åŠ¡
- [ ] å®ç°ç”Ÿæˆå¾ªç¯
- [ ] EOS æ£€æµ‹
- [ ] æœ€å¤§é•¿åº¦é™åˆ¶

```rust
// src/models/qwen3_tts/generate.rs

pub fn generate_autoregressive(
    model: &mut TalkerModel,
    input_ids: &[u32],
    config: &GenerationConfig,
) -> Result<Vec<u32>> {
    let mut tokens = input_ids.to_vec();
    let mut kv_caches = model.new_kv_caches(config.max_new_tokens);
    let mut ctx = SamplingContext::new(config.seed);
    
    for _ in 0..config.max_new_tokens {
        // Forward pass
        let logits = model.forward(&tokens, &mut kv_caches)?;
        
        // Get last token logits
        let last_logits = logits.i((0, logits.dim(1)? - 1))?;
        
        // Sample next token
        let next_token = if config.temperature == 0.0 {
            argmax(&last_logits)?
        } else {
            sample(&last_logits, config, &mut ctx, &tokens)?
        };
        
        // Append token
        tokens.push(next_token);
        
        // Check EOS
        if Some(next_token) == config.eos_token_id {
            break;
        }
    }
    
    Ok(tokens)
}
```

### 2.3 æµå¼è¾“å‡ºæ”¯æŒ

#### ä»»åŠ¡
- [ ] å®ç°æµå¼ç”Ÿæˆ
- [ ] éŸ³é¢‘å—å›è°ƒ
- [ ] ä½å»¶è¿Ÿä¼˜åŒ–

```rust
// src/models/qwen3_tts/streaming.rs

pub type AudioCallback = Box<dyn FnMut(&[f32]) -> Result<()> + Send>;

pub fn generate_streaming(
    pipeline: &Qwen3TTSPipeline,
    text_ids: &[u32],
    config: &GenerationConfig,
    mut callback: AudioCallback,
) -> Result<()> {
    let chunk_size = 50; // frames per chunk
    
    // Generate in chunks
    for chunk in (0..config.max_new_tokens).step_by(chunk_size) {
        // Generate chunk
        let chunk_tokens = &text_ids[chunk..chunk + chunk_size];
        let semantic_tokens = pipeline.talker.generate(chunk_tokens, None, config)?;
        
        // Convert to audio
        let acoustic_codes = pipeline.code_predictor.generate(&semantic_tokens)?;
        let audio = pipeline.decoder.decode(&acoustic_codes)?;
        
        // Callback
        callback(&audio)?;
    }
    
    Ok(())
}
```

---

## é˜¶æ®µ 3: Tokenizer é›†æˆ (200 è¡Œ)

### 3.1 HuggingFace Tokenizers

#### ä»»åŠ¡
- [ ] é›†æˆ tokenizers crate
- [ ] åŠ è½½ Qwen2 tokenizer
- [ ] å¤šè¯­è¨€æ”¯æŒ

```rust
// src/models/qwen3_tts/tokenizer.rs

use tokenizers::{Tokenizer, Encoding};

pub struct QwenTokenizer {
    tokenizer: Tokenizer,
}

impl QwenTokenizer {
    pub fn from_pretrained(model_id: &str) -> Result<Self> {
        let tokenizer_path = download_tokenizer(model_id)?;
        let tokenizer = Tokenizer::from_file(tokenizer_path)?;
        Ok(Self { tokenizer })
    }

    pub fn encode(&self, text: &str) -> Result<Vec<u32>> {
        let encoding: Encoding = self.tokenizer.encode(text, true)?;
        Ok(encoding.get_ids().to_vec())
    }

    pub fn decode(&self, ids: &[u32]) -> Result<String> {
        let text = self.tokenizer.decode(ids, true)?;
        Ok(text)
    }
}
```

### 3.2 è¯­éŸ³ Tokenizer

#### ä»»åŠ¡
- [ ] å®ç°è¯­éŸ³ç¼–ç å™¨
- [ ] æ”¯æŒ 16 ç æœ¬
- [ ] éŸ³é¢‘â†’ç æœ¬è½¬æ¢

```rust
// src/models/qwen3_tts/speech_tokenizer.rs

pub struct SpeechTokenizer {
    encoder: Encoder12Hz,
    device: Device,
}

impl SpeechTokenizer {
    pub fn encode(&self, audio: &[f32]) -> Result<Vec<Vec<u32>>> {
        // Convert audio to codes
        let audio_tensor = Tensor::from_vec(audio.to_vec(), (1, audio.len()), &self.device)?;
        let codes = self.encoder.encode(&audio_tensor)?;
        
        // Reshape to [seq, 16 codebooks]
        let codes_vec: Vec<u32> = codes.flatten_all()?.to_vec1()?;
        let seq_len = codes_vec.len() / 16;
        
        let mut result = Vec::with_capacity(seq_len);
        for i in 0..seq_len {
            let frame = codes_vec[i * 16..(i + 1) * 16].to_vec();
            result.push(frame);
        }
        
        Ok(result)
    }
}
```

---

## é˜¶æ®µ 4: æ€§èƒ½ä¼˜åŒ– (400 è¡Œ)

### 4.1 FlashAttention 2

#### ä»»åŠ¡
- [ ] é›†æˆ candle-flash-attn
- [ ] æ¡ä»¶ç¼–è¯‘æ”¯æŒ
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

```toml
# Cargo.toml
[features]
flash-attn = ["cuda", "dep:candle-flash-attn"]

[dependencies]
candle-flash-attn = { version = "0.9", optional = true }
```

```rust
// src/models/qwen3_tts/attention.rs

#[cfg(feature = "flash-attn")]
use candle_flash_attn::flash_attn;

#[cfg(feature = "flash-attn")]
pub fn flash_attention_forward(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    softmax_scale: f32,
) -> Result<Tensor> {
    flash_attn(q, k, v, softmax_scale, true)
}

#[cfg(not(feature = "flash-attn"))]
pub fn flash_attention_forward(
    q: &Tensor,
    k: &Tensor,
    v: &Tensor,
    softmax_scale: f32,
) -> Result<Tensor> {
    // Fallback to standard attention
    standard_attention(q, k, v, softmax_scale)
}
```

### 4.2 KV ç¼“å­˜ä¼˜åŒ–

#### ä»»åŠ¡
- [ ] é¢„åˆ†é… KV ç¼“å­˜
- [ ] åŸåœ°æ›´æ–°
- [ ] å‡å°‘å†…å­˜åˆ†é…

```rust
// src/models/qwen3_tts/kv_cache_optimized.rs

pub struct OptimizedKVCache {
    k_cache: Tensor,  // Pre-allocated [max_seq, num_kv_heads, head_dim]
    v_cache: Tensor,
    current_pos: usize,
}

impl OptimizedKVCache {
    pub fn new(max_seq: usize, num_kv_heads: usize, head_dim: usize, device: &Device) -> Result<Self> {
        let k_cache = Tensor::zeros((max_seq, num_kv_heads, head_dim), DType::F32, device)?;
        let v_cache = Tensor::zeros((max_seq, num_kv_heads, head_dim), DType::F32, device)?;
        
        Ok(Self {
            k_cache,
            v_cache,
            current_pos: 0,
        })
    }

    pub fn update_inplace(&mut self, k_new: &Tensor, v_new: &Tensor) -> Result<()> {
        // Inplace update using slice_assign
        let seq_len = k_new.dim(0)?;
        
        self.k_cache = self.k_cache.slice_assign(
            &[self.current_pos..self.current_pos + seq_len, 0.., 0..],
            k_new,
        )?;
        self.v_cache = self.v_cache.slice_assign(
            &[self.current_pos..self.current_pos + seq_len, 0.., 0..],
            v_new,
        )?;
        
        self.current_pos += seq_len;
        Ok(())
    }
}
```

### 4.3 æ‰¹å¤„ç†æ”¯æŒ

#### ä»»åŠ¡
- [ ] å®ç°æ‰¹å¤„ç†æ¨ç†
- [ ] åŠ¨æ€æ‰¹å¤„ç†
- [ ] ååé‡ä¼˜åŒ–

```rust
// src/models/qwen3_tts/batch.rs

pub fn batch_synthesize(
    pipeline: &Qwen3TTSPipeline,
    texts: &[Vec<u32>],
    config: &GenerationConfig,
) -> Result<Vec<Vec<f32>>> {
    let batch_size = texts.len();
    let mut results = Vec::with_capacity(batch_size);
    
    // Pad sequences to same length
    let max_len = texts.iter().map(|t| t.len()).max().unwrap();
    let padded_texts: Vec<Vec<u32>> = texts
        .iter()
        .map(|t| {
            let mut padded = t.clone();
            padded.extend(vec![0; max_len - t.len()]);
            padded
        })
        .collect();
    
    // Batch forward
    let batch_tensor = Tensor::from_vec(
        padded_texts.concat(),
        (batch_size, max_len),
        &pipeline.device,
    )?;
    
    let semantic_tokens = pipeline.talker.batch_generate(&batch_tensor, config)?;
    
    // Process each sequence
    for i in 0..batch_size {
        let seq_tokens = semantic_tokens.i((i, ..))?;
        let acoustic_codes = pipeline.code_predictor.generate(&seq_tokens)?;
        let audio = pipeline.decoder.decode(&acoustic_codes)?;
        results.push(audio);
    }
    
    Ok(results)
}
```

---

## é˜¶æ®µ 5: æµå¼æ¨ç† (300 è¡Œ)

### 5.1 Dual-Track æµå¼æ¶æ„

#### ä»»åŠ¡
- [ ] å®ç°åŒæµæ¶æ„
- [ ] æ–‡æœ¬æµå¤„ç†
- [ ] éŸ³é¢‘æµè¾“å‡º

```rust
// src/models/qwen3_tts/streaming_dual.rs

pub struct DualTrackStreamer {
    text_buffer: Vec<u32>,
    audio_buffer: Vec<f32>,
    pipeline: Arc<Qwen3TTSPipeline>,
}

impl DualTrackStreamer {
    pub fn new(pipeline: Arc<Qwen3TTSPipeline>) -> Self {
        Self {
            text_buffer: Vec::new(),
            audio_buffer: Vec::new(),
            pipeline,
        }
    }

    pub fn push_text(&mut self, text_ids: &[u32]) -> Result<()> {
        self.text_buffer.extend(text_ids);
        Ok(())
    }

    pub fn generate_audio(&mut self, config: &GenerationConfig) -> Result<Vec<f32>> {
        if self.text_buffer.is_empty() {
            return Ok(Vec::new());
        }
        
        // Generate from buffered text
        let semantic_tokens = self.pipeline.talker.generate(
            &self.text_buffer,
            None,
            config,
        )?;
        
        let acoustic_codes = self.pipeline.code_predictor.generate(&semantic_tokens)?;
        let audio = self.pipeline.decoder.decode(&acoustic_codes)?;
        
        self.text_buffer.clear();
        self.audio_buffer.extend(&audio);
        
        Ok(audio)
    }

    pub fn get_audio(&mut self) -> Vec<f32> {
        let audio = self.audio_buffer.clone();
        self.audio_buffer.clear();
        audio
    }
}
```

### 5.2 ä½å»¶è¿Ÿä¼˜åŒ–

#### ä»»åŠ¡
- [ ] å¢é‡ç”Ÿæˆ
- [ ] é¦–åŒ…ä¼˜åŒ–
- [ ] å»¶è¿Ÿç›‘æ§

```rust
// src/models/qwen3_tts/latency.rs

pub struct LatencyMonitor {
    start_time: Instant,
    first_token_time: Option<Instant>,
    first_audio_time: Option<Instant>,
}

impl LatencyMonitor {
    pub fn new() -> Self {
        Self {
            start_time: Instant::now(),
            first_token_time: None,
            first_audio_time: None,
        }
    }

    pub fn record_first_token(&mut self) {
        self.first_token_time = Some(Instant::now());
    }

    pub fn record_first_audio(&mut self) {
        self.first_audio_time = Some(Instant::now());
    }

    pub fn get_time_to_first_token(&self) -> Duration {
        self.first_token_time.unwrap() - self.start_time
    }

    pub fn get_time_to_first_audio(&self) -> Duration {
        self.first_audio_time.unwrap() - self.start_time
    }
}
```

---

## æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
cargo test --lib

# è¿è¡Œæƒé‡åŠ è½½æµ‹è¯•
cargo test --lib weights

# è¿è¡Œæ¨ç†æµ‹è¯•
cargo test --lib inference
```

### é›†æˆæµ‹è¯•

```bash
# è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
cargo test --test e2e

# è¿è¡Œæ€§èƒ½æµ‹è¯•
cargo bench
```

### æ€§èƒ½åŸºå‡†

```bash
# CPU åŸºå‡†
cargo bench --no-default-features --features cpu

# CUDA åŸºå‡†
cargo bench --features cuda
```

---

## æ—¶é—´ä¼°ç®—

| é˜¶æ®µ | ä»»åŠ¡ | ä»£ç è¡Œæ•° | æ—¶é—´ |
|------|------|---------|------|
| 1 | æƒé‡åŠ è½½ | 500 | 3-4 å¤© |
| 2 | æ¨ç†å¾ªç¯ | 300 | 2-3 å¤© |
| 3 | Tokenizer | 200 | 2 å¤© |
| 4 | æ€§èƒ½ä¼˜åŒ– | 400 | 4-5 å¤© |
| 5 | æµå¼æ¨ç† | 300 | 3-4 å¤© |
| **æ€»è®¡** | | **1,700** | **14-18 å¤©** |

---

## æˆåŠŸæ ‡å‡†

- [ ] å®Œæ•´æ¨ç†å¾ªç¯å·¥ä½œ
- [ ] ç”Ÿæˆæ¸…æ™°éŸ³é¢‘
- [ ] RTF < 1.0 (GPU)
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´
- [ ] ç¤ºä¾‹å¯è¿è¡Œ

---

**åˆ›å»ºæ—¥æœŸ**: 2026 å¹´ 2 æœˆ 21 æ—¥  
**ç‰ˆæœ¬**: 1.0.0
